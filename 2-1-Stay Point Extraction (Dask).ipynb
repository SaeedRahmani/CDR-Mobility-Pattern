{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stay Point Extraction\n",
    "\n",
    "======================================================\n",
    "##### Using this code, we filtered the raw call details records (CDR) data and removed the noises in the data in order to identify people stop locations. In addition, we have developed an algorithm to find the individuals' home and work location based on the frequency of their stop points on different times of the day\n",
    "======================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import haversine_distances         \n",
    "from geopy.distance import geodesic                              \n",
    "import numpy as np                                               \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import pandas as pd \n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=6, \n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='2GB')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callsDataset = dd.read_csv(r'E:\\***\\raw_data.csv', parse_dates=['Date_Time'],dtype={'Lat':'float','Long':'float','ID':'int'})\n",
    "callsDataset = callsDataset.set_index('Date_Time')\n",
    "callsDataset = callsDataset.reset_index()\n",
    "\n",
    "\n",
    "shapeFile = gpd.read_file(r'E:\\***\\tehran.shp')\n",
    "shapeFile = shapeFile.drop('Area',axis=1)\n",
    "\n",
    "callsDatasetgp = callsDataset.groupby('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medoid(candidateSet=[]):\n",
    "    \"\"\"\n",
    "    This func use sklearn pairwise matrices to calculate medoid for each canidateSet.\n",
    "    A candidateSet is a set of points for each person which seems to be stop points\n",
    "    based on the short distance criteria and the minimum activity period\n",
    "    \"\"\"\n",
    "                                       \n",
    "    l = [candidateSet[i][2:4] * (np.pi / 180) for i in range(len(candidateSet))]\n",
    "    medIndex = np.argmin((haversine_distances(l) * 6731000).sum(axis=1)) \n",
    "                                                               \n",
    "    cSetMedoid = candidateSet[medIndex]\n",
    "    cSetMedoid = np.append(cSetMedoid,[candidateSet[0][0],candidateSet[-1][0] - candidateSet[0][0]])\n",
    "    \n",
    "    l.clear()\n",
    "    \n",
    "    return cSetMedoid\n",
    "\n",
    "\n",
    "def agg_cluster(stopPoints=[]):\n",
    "    \"\"\"                             \n",
    "    This function will use sklearn hierarchical clustering to cluster the stop points into stayPoints.\n",
    "    For example, two home location in the morning and in the evening (stopPoints) are clustered into one home location (stayPoint)\n",
    "    \"\"\"     \n",
    "    stayPointsTemp = []                       \n",
    "    stopPointsModifiedTemp = pd.DataFrame(stopPoints,columns=['Date_Time','ID','Lat','Long','StayStart','Duration'])\n",
    "    \n",
    "    if len(stopPoints)>1: \n",
    "        dist = haversine_distances(stopPointsModifiedTemp[['Lat','Long']]) * (np.pi/180) * 6731000\n",
    "        Agg = AgglomerativeClustering(n_clusters=None, affinity='precomputed', linkage='average', distance_threshold=1000).fit(dist)                                                                                                                                                            \n",
    "        stopPointsModifiedTemp['cluster'] = Agg.labels_                                                             \n",
    "\n",
    "        for clusterNo in stopPointsModifiedTemp.cluster.unique():                                                    #this process will find medoid of each final cluster and then append medoids lat/long to the same clusters\n",
    "            clusterN = stopPointsModifiedTemp.loc[stopPointsModifiedTemp.cluster == clusterNo][['Lat','Long','Date_Time']]\n",
    "            medIndex = np.argmin(haversine_distances(clusterN[['Lat','Long']]).sum(axis=1))\n",
    "            medClusterN = clusterN.iloc[medIndex]\n",
    "            stayPointsTemp.append(medClusterN)   \n",
    "            stopPointsModifiedTemp.loc[(stopPointsModifiedTemp.cluster == clusterNo),'Lat'] = medClusterN.Lat # WHY TWO =?\n",
    "            stopPointsModifiedTemp.loc[(stopPointsModifiedTemp.cluster == clusterNo),'Long'] = medClusterN.Long # HERE WE CAN HAVE ANOTHER VARIABLE FOR UNIQUE POINTS WHICH ARE CLUSTERED. BUT, WE ARE JUST CHAGING THE LATLONG IN THE BASE DATASET\n",
    "        \n",
    "        stopPointsModifiedTemp = gpd.GeoDataFrame(stopPointsModifiedTemp, geometry=gpd.points_from_xy(x=stopPointsModifiedTemp.Long, y=stopPointsModifiedTemp.Lat), crs={'init': 'epsg:4326'}) #$NEW use geopandas to create a geo DataFrame\n",
    "        stopPointsModifiedTemp = gpd.sjoin(stopPointsModifiedTemp, shapeFile, how='left', op='within') # Spatial join\n",
    "        stopPointsModifiedTemp.drop(columns=['index_right', 'geometry'], inplace=True)                 # Drop sth which is not needed\n",
    "        stopPointsModifiedTemp.fillna(float(0), inplace=True)                                                     #This is for filling points which are not in Tehran shapeFile\n",
    "        \n",
    "        return stopPointsModifiedTemp\n",
    "        \n",
    "    else:\n",
    "        return stopPointsModifiedTemp\n",
    "\n",
    "\n",
    "def HWO_finder(notLabeledStopPoints):\n",
    "    \"\"\"\n",
    "    This function allocates home, work, and other labels to the stay points. At this stage,\n",
    "    the inpute argument is the stayPoints list because we need the ferquency at which \n",
    "    a stay location has been observed. \n",
    "    \"\"\"\n",
    "    \n",
    "    labeledStopPoints = notLabeledStopPoints \n",
    "    labeledStopPoints['Day'] = labeledStopPoints['StayStart'].dt.day_name()                              \n",
    "    labeledStopPoints['Week'] = labeledStopPoints['StayStart'].dt.week                    \n",
    "    labeledStopPoints.set_index('StayStart', inplace=True)                            \n",
    "    labeledStopPoints['Purpose'] = np.nan                                                \n",
    "    \n",
    "    homeLoc = labeledStopPoints.loc[(labeledStopPoints.Day != 'Friday')][['Lat','Long','Duration']].between_time('19:00', '7:00').mode()#Find most used location as Home\n",
    "    if len(homeLoc)>=1:\n",
    "        homeLoc = homeLoc.dropna()\n",
    "        homeLoc = (homeLoc.groupby(['Lat','Long'],as_index = False).sum()).max()\n",
    "        #homeLoc  = max(homeLoc.groupby(['Lat','Long'],as_index = False).sum())\n",
    "        #print(homeLoc)\n",
    "        labeledStopPoints.loc[((labeledStopPoints.Lat.isin([homeLoc.Lat])) & (labeledStopPoints.Long.isin([homeLoc.Long]))),'Purpose']='Home'                \n",
    "    \n",
    "    workLoc = labeledStopPoints.loc[(labeledStopPoints.Purpose != 'Home') & (labeledStopPoints.Day != 'Friday')][['Lat','Long']].between_time('7:00', '19:00').mode()  #Find most used location as Work. excluding Home\n",
    "    if len(workLoc)>=1:\n",
    "        workLoc = workLoc.dropna()\n",
    "        workLoc = (workLoc.groupby(['Lat','Long'],as_index = False).sum()).max()\n",
    "        print(workLoc)\n",
    "        labeledStopPoints.loc[((labeledStopPoints.Lat.isin([workLoc.Lat])) & (labeledStopPoints.Long.isin([workLoc.Long]))), 'Purpose']='Work'   \n",
    "    \n",
    "    if ((labeledStopPoints.loc[(labeledStopPoints.Purpose == 'Work')].shape[0]) / (len(labeledStopPoints.Week.unique()))) < 1:  #New                                                                           #to get average trip frequency for five week \n",
    "        labeledStopPoints.loc[(labeledStopPoints.Purpose == 'Work'), 'Purpose'] = np.nan\n",
    "    \n",
    "    labeledStopPoints.loc[((labeledStopPoints.Purpose != 'Work') & (labeledStopPoints.Purpose != 'Home')), 'Purpose'] = 'Other'     \n",
    "    return labeledStopPoints \n",
    "\n",
    "\n",
    "\n",
    "def dataclust(uniqIdCalls):\n",
    "    \n",
    "    uniqIdCalls=uniqIdCalls.to_numpy()\n",
    "    candidateSet = []                                               # For storing the candidate set for each person. A candidate set is a set of points that are locally close to each other.\n",
    "    stopPoints = []                                                 # Stop points are the centroid (medoid) of candidate sets keeping in mind that the first and the last points in the candidate sets should have a minimum time difference\n",
    "    allCandidateSets = []                                           # For storing all candidate sets of a person\n",
    "    candidateSet.append(uniqIdCalls[0])   \n",
    "    \n",
    "    for i in range(len(uniqIdCalls) - 1):                               \n",
    "        if geodesic(uniqIdCalls[i][2:4], uniqIdCalls[i+1][2:4]).meters <=500:   \n",
    "            candidateSet.append(uniqIdCalls[i+1])   \n",
    "        else:\n",
    "            if (candidateSet[-1][0] - candidateSet[0][0]).seconds > 600:                     \n",
    "                stopPoints.append(medoid(candidateSet))\n",
    "                allCandidateSets.append(candidateSet)\n",
    "            candidateSet = []\n",
    "            candidateSet.append(uniqIdCalls[i+1])\n",
    "    if (candidateSet[-1][0] - candidateSet[0][0]).seconds > 600:  #append final cluster\n",
    "       stopPoints.append(medoid(candidateSet))\n",
    "       allCandidateSets.append(candidateSet)\n",
    "    if len(stopPoints)>=1:     \n",
    "        stopPointsModified = agg_cluster(stopPoints)\n",
    "        stopPointsLabeled = HWO_finder(stopPointsModified)\n",
    "        if ((stopPointsLabeled.loc[(stopPointsLabeled.Purpose == 'Home')].shape[0]) / (len(stopPointsLabeled.Week.unique()))) > 1:\n",
    "        #stopPointsLabeled.reset_index(inplace=True)\n",
    "            Home=stopPointsLabeled.loc[stopPointsLabeled.Purpose=='Home'].head(1)\n",
    "            #usersCDRTract[int(Home['manategh'])]+=1\n",
    "        else:\n",
    "            stopPointsLabeled= pd.DataFrame(columns=['Date_Time','ID','Lat','Long','Duration','cluster', 'manategh','Day','Week','Purpose'])\n",
    "    else:\n",
    "        stopPointsLabeled= pd.DataFrame(columns=['Date_Time','ID','Lat','Long','Duration','cluster', 'manategh','Day','Week','Purpose'])\n",
    "\n",
    "    return stopPointsLabeled\n",
    "\n",
    "\n",
    "\n",
    "def CDR(x):\n",
    "    Home = stay_points.loc[stay_points.Purpose == 'Home'].head(1)\n",
    "    usersCDRTracts[int(Home['manategh'])] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stay_points = callsDatasetgp.apply(dataclust,meta={'Date_Time':'f8','ID':'int','Lat':'float','Long':'float','Duration':'f8','cluster':'int', 'manategh':'int','Day':'f8','Week':'int','Purpose':'f8'}).compute()\n",
    "stay_points.to_csv('E:stay_points.csv',index = True)\n",
    "\n",
    "stay_points = stay_points.droplevel('ID').reset_index()\n",
    "stay_points_gp = np.groupby('ID')\n",
    "\n",
    "usersCDRTracts=np.zeros((23,1))\n",
    "stay_points_gp.apply(CDR)\n",
    "\n",
    "CDR_tract_pop = pd.DataFrame(usersCDRTracts,columns=['Pop'])\n",
    "CDR_tract_pop.to_csv('E:CDR_tract_pop.csv' , index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
