{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\distributed\\dashboard\\core.py:79: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    }
   ],
   "source": [
    "cluster = LocalCluster(n_workers=6, \n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='2GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f3e837dea3437792df492508da6655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>LocalCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:12787</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:12790/status' target='_blank'>http://127.0.0.1:12790/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>6</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>12.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:12787' processes=6 threads=12, memory=12.00 GB>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import haversine_distances         # Import this for pairwise distances to get medoids\n",
    "from geopy.distance import geodesic                              # This will calculate haversine to meters\n",
    "import numpy as np                                               # This is useful for obtaining medoid index\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callsDataset = dd.read_csv(r'E:\\Arshad\\Thesis\\Datasets\\20000Data\\Raw data\\data20000fix.csv', parse_dates=['Date_Time'],dtype={'Lat':'float','Long':'float','ID':'int'})\n",
    "#callsDataset = callsDataset.repartition(npartitions=5)\n",
    "callsDataset = callsDataset.set_index('Date_Time')\n",
    "callsDataset = callsDataset.reset_index()\n",
    "\n",
    "\n",
    "shapeFile = gpd.read_file(r'E:\\Arshad\\Thesis\\Datasets\\Shapefiles\\Tehran manategh\\tehran.shp')\n",
    "shapeFile = shapeFile.drop('Area',axis=1)\n",
    "\n",
    "callsDatasetgp = callsDataset.groupby('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def medoid(candidateSet=[]):\n",
    "    \"\"\"\n",
    "    This func use sklearn pairwise matrices to calculate medoid for each canidateSet.\n",
    "    A candidateSet is a set of points for each person which semms to be stop points\n",
    "    based on the short distance criteria and the minimum activity period\n",
    "    \"\"\"\n",
    "                                       \n",
    "    # Convert coordinate to radians / IS IT BETTER TO APPEND THE RADIANS AS NEW FIELDS TO SAVE MEMRORY AND TO HAVE THEM FOR LATER CALs?\n",
    "    l = [candidateSet[i][2:4] * (np.pi / 180) for i in range(len(candidateSet))]                                                          # CAN'T WE CHANGE THEM IN PLACE TO SAVE THE MEMORY?\n",
    "    medIndex = np.argmin((haversine_distances(l) * 6731000).sum(axis=1)) # Harversine calculates the distance between two points in Kilometers\n",
    "                                                                # and Argmin return the index of the minimum value in an array\n",
    "                                                                # So with this line of code, we want to find the index of the point with minimum distance with others\n",
    "    cSetMedoid = candidateSet[medIndex]\n",
    "    cSetMedoid = np.append(cSetMedoid,[candidateSet[0][0],candidateSet[-1][0] - candidateSet[0][0]])\n",
    "    \n",
    "    l.clear()\n",
    "    \n",
    "    \n",
    "    return cSetMedoid\n",
    "\n",
    "\n",
    "def agg_cluster(stopPoints=[]):\n",
    "    \"\"\"                             \n",
    "    This function will use sklearn hierarchical clustering to cluster the stop points into stayPoints.\n",
    "    For example, two home location in the morning and in the evening (stopPoints) are clustered into one home location (stayPoint)\n",
    "    \"\"\"\n",
    "        \n",
    "    stayPointsTemp = []                       \n",
    "    stopPointsModifiedTemp = pd.DataFrame(stopPoints,columns=['Date_Time','ID','Lat','Long','StayStart','Duration'])\n",
    "    \n",
    "    if len(stopPoints)>1: \n",
    "        dist = haversine_distances(stopPointsModifiedTemp[['Lat','Long']]) * (np.pi/180) * 6731000 # ADD COMMENT\n",
    "       \n",
    "        Agg = AgglomerativeClustering(n_clusters=None, affinity='precomputed', linkage='average', distance_threshold=1000).fit(dist) #$New \n",
    "                                                                                                    # https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering\n",
    "                                                                                                    # https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "                                                                                                    # The parameters need modifications. e.g., the linkage = 'complete' / AND WHY NOT DBSCAN                                                                         \n",
    "        stopPointsModifiedTemp['cluster'] = Agg.labels_                                                             #add cluster index to final cluster\n",
    "     \n",
    "    \n",
    "        for clusterNo in stopPointsModifiedTemp.cluster.unique():                                                    #this process will find medoid of each final cluster and then append medoids lat/long to the same clusters\n",
    "            clusterN = stopPointsModifiedTemp.loc[stopPointsModifiedTemp.cluster == clusterNo][['Lat','Long','Date_Time']]\n",
    "            medIndex = np.argmin(haversine_distances(clusterN[['Lat','Long']]).sum(axis=1))\n",
    "            medClusterN = clusterN.iloc[medIndex]\n",
    "            stayPointsTemp.append(medClusterN)   \n",
    "            stopPointsModifiedTemp.loc[(stopPointsModifiedTemp.cluster == clusterNo),'Lat'] = medClusterN.Lat # WHY TWO =?\n",
    "            stopPointsModifiedTemp.loc[(stopPointsModifiedTemp.cluster == clusterNo),'Long'] = medClusterN.Long # HERE WE CAN HAVE ANOTHER VARIABLE FOR UNIQUE POINTS WHICH ARE CLUSTERED. BUT, WE ARE JUST CHAGING THE LATLONG IN THE BASE DATASET\n",
    "        \n",
    "        stopPointsModifiedTemp = gpd.GeoDataFrame(stopPointsModifiedTemp, geometry=gpd.points_from_xy(x=stopPointsModifiedTemp.Long, y=stopPointsModifiedTemp.Lat), crs={'init': 'epsg:4326'}) #$NEW use geopandas to create a geo DataFrame\n",
    "        stopPointsModifiedTemp = gpd.sjoin(stopPointsModifiedTemp, shapeFile, how='left', op='within') # Spatial join\n",
    "        stopPointsModifiedTemp.drop(columns=['index_right', 'geometry'], inplace=True)                 # Drop sth which is not needed\n",
    "        stopPointsModifiedTemp.fillna(float(0), inplace=True)                                                     #This is for filling points which are not in Tehran shapeFile\n",
    "        \n",
    "        \n",
    "        return stopPointsModifiedTemp\n",
    "        \n",
    "    else:\n",
    "        return stopPointsModifiedTemp\n",
    "\n",
    "\n",
    "def HWO_finder(notLabeledStopPoints):\n",
    "    \"\"\"\n",
    "    This function allocates home, work, and other labels to the stay points. At this stage,\n",
    "    the inpute argument is the stayPoints list because we need the ferquency at which \n",
    "    a stay location has been observed. \n",
    "    \"\"\"\n",
    "    \n",
    "    labeledStopPoints = notLabeledStopPoints # These variables will be removed later. They are here just for clarification now!\n",
    "    labeledStopPoints['Day'] = labeledStopPoints['StayStart'].dt.day_name()               #Extract Day name                  \n",
    "    labeledStopPoints['Week'] = labeledStopPoints['StayStart'].dt.week                    #Extract Week name\n",
    "    labeledStopPoints.set_index('StayStart', inplace=True)                             #Set index as Start Stay to use between Func\n",
    "    labeledStopPoints['Purpose'] = np.nan                                                #Create column of empty value\n",
    "    #labeledStopPoints['TypeDay']=np.nan\n",
    "    ########\n",
    "    \n",
    "    homeLoc = labeledStopPoints.loc[(labeledStopPoints.Day != 'Friday')][['Lat','Long','Duration']].between_time('19:00', '7:00').mode()#Find most used location as Home\n",
    "    if len(homeLoc)>=1:\n",
    "        homeLoc = homeLoc.dropna()\n",
    "        homeLoc = (homeLoc.groupby(['Lat','Long'],as_index = False).sum()).max()\n",
    "        #homeLoc  = max(homeLoc.groupby(['Lat','Long'],as_index = False).sum())\n",
    "        #print(homeLoc)\n",
    "        labeledStopPoints.loc[((labeledStopPoints.Lat.isin([homeLoc.Lat])) & (labeledStopPoints.Long.isin([homeLoc.Long]))),'Purpose']='Home'                \n",
    "    ##add frequencies \n",
    "    \n",
    "    #homeLoc  = max(homeLoc.groupby(['Lat','Long'],as_index = False).sum())\n",
    "    #labeledStopPoints.loc[((labeledStopPoints.Lat.isin(homeLoc.Lat)) & (labeledStopPoints.Long.isin(homeLoc.Long))),'Purpose']='Home'                \n",
    "    ##add frequencies \n",
    "    \n",
    "    workLoc = labeledStopPoints.loc[(labeledStopPoints.Purpose != 'Home') & (labeledStopPoints.Day != 'Friday')][['Lat','Long']].between_time('7:00', '19:00').mode()  #Find most used location as Work. excluding Home\n",
    "    if len(workLoc)>=1:\n",
    "        workLoc = workLoc.dropna()\n",
    "        workLoc = (workLoc.groupby(['Lat','Long'],as_index = False).sum()).max()\n",
    "        print(workLoc)\n",
    "        labeledStopPoints.loc[((labeledStopPoints.Lat.isin([workLoc.Lat])) & (labeledStopPoints.Long.isin([workLoc.Long]))), 'Purpose']='Work'   \n",
    "    #labeledStopPoints.loc[((labeledStopPoints.Lat.isin(workLoc.Lat)) & (labeledStopPoints.Long.isin(workLoc.Long))), 'Purpose']='Work'   \n",
    "    \n",
    "    if ((labeledStopPoints.loc[(labeledStopPoints.Purpose == 'Work')].shape[0]) / (len(labeledStopPoints.Week.unique()))) < 1:  #New                                                                           #to get average trip frequency for five week \n",
    "        labeledStopPoints.loc[(labeledStopPoints.Purpose == 'Work'), 'Purpose'] = np.nan\n",
    "    \n",
    "    labeledStopPoints.loc[((labeledStopPoints.Purpose != 'Work') & (labeledStopPoints.Purpose != 'Home')), 'Purpose'] = 'Other' \n",
    "    \n",
    "                                            #Nan value to other\n",
    "    #labeledStopPoints.drop(columns=['Day'], inplace=True)      #&NEW   drop sth which is not needed\n",
    "    \n",
    "    return labeledStopPoints \n",
    "\n",
    "\n",
    "\n",
    "def dataclust(uniqIdCalls):\n",
    "    \n",
    "    uniqIdCalls=uniqIdCalls.to_numpy()\n",
    "    \n",
    "    \n",
    "    candidateSet = []                                               # For storing the candidate set for each person. A candidate set is a set of points that are locally close to each other.\n",
    "    stopPoints = []                                                 # Stop points are the centroid (medoid) of candidate sets keeping in mind that the first and the last points in the candidate sets should have a minimum time difference\n",
    "    allCandidateSets = []                                           # For storing all candidate sets of a person\n",
    "    \n",
    "    candidateSet.append(uniqIdCalls[0])   #using iloc will slice dataset by using rows true index. we can use advantages of lists and Pandas series \n",
    "    \n",
    "    for i in range(len(uniqIdCalls) - 1):                                #this will iterate row by row of data frame which is a tuple and I use 0 index to get number of rows\n",
    "        if geodesic(uniqIdCalls[i][2:4], uniqIdCalls[i+1][2:4]).meters <=500:   #This will combine geopy and data frame row by row indexes for getting staypoint\n",
    "            candidateSet.append(uniqIdCalls[i+1])   \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            if (candidateSet[-1][0] - candidateSet[0][0]).seconds > 600:                      #Use pandas timestamps to get durations \n",
    "                \n",
    "                stopPoints.append(medoid(candidateSet)) # WE SHOULD ALSO KEEP THE INITIAL DATASET BECAUSE WE MIGHT NEED THEM IN FUTURE. WE CAN APPEND ONLY STAYPOINT AND LCUSTERS LABELS TO THE INITIAL DATASET\n",
    "                \n",
    "                allCandidateSets.append(candidateSet)\n",
    "            candidateSet = []\n",
    "            candidateSet.append(uniqIdCalls[i+1])\n",
    "    \n",
    "    \n",
    "    if (candidateSet[-1][0] - candidateSet[0][0]).seconds > 600:  #append final cluster\n",
    "       stopPoints.append(medoid(candidateSet))\n",
    "       allCandidateSets.append(candidateSet)\n",
    "       \n",
    "    \n",
    "    if len(stopPoints)>=1:     \n",
    "        stopPointsModified = agg_cluster(stopPoints)\n",
    "        stopPointsLabeled = HWO_finder(stopPointsModified)\n",
    "        if ((stopPointsLabeled.loc[(stopPointsLabeled.Purpose == 'Home')].shape[0]) / (len(stopPointsLabeled.Week.unique()))) > 1:\n",
    "        #stopPointsLabeled.reset_index(inplace=True)\n",
    "            Home=stopPointsLabeled.loc[stopPointsLabeled.Purpose=='Home'].head(1)\n",
    "            #usersCDRTract[int(Home['manategh'])]+=1\n",
    "        else:\n",
    "            stopPointsLabeled= pd.DataFrame(columns=['Date_Time','ID','Lat','Long','Duration','cluster', 'manategh','Day','Week','Purpose'])\n",
    "    else:\n",
    "        stopPointsLabeled= pd.DataFrame(columns=['Date_Time','ID','Lat','Long','Duration','cluster', 'manategh','Day','Week','Purpose'])\n",
    "           \n",
    "    \n",
    "   \n",
    "    return stopPointsLabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = callsDatasetgp.apply(dataclust,meta={'Date_Time':'f8','ID':'int','Lat':'float','Long':'float','Duration':'f8','cluster':'int', 'manategh':'int','Day':'f8','Week':'int','Purpose':'f8'}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv('E:stay20000Onemonth.csv',index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = x.droplevel('ID').reset_index()\n",
    "df = np.groupby('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as npp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersCDRTracts=np.zeros((23,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CDR(x):\n",
    "    Home=x.loc[x.Purpose=='Home'].head(1)\n",
    "    usersCDRTracts[int(Home['manategh'])]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(CDR)\n",
    "usersCDRTracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMonth20000tractpop = pd.DataFrame(usersCDRTracts,columns=['Pop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMonth20000tractpop.to_csv('E:oneMonth20000tractpop.csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
