{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import geopandas as gpd\n",
    "import datetime as dt\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(cell):\n",
    "    \"\"\"\n",
    "    This function add noise to the input variable\n",
    "    \n",
    "    input: a number--fload or integer\n",
    "    output: a noisy number\n",
    "    \"\"\"\n",
    "    cell = cell + 0.005*np.random.randn()\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_(epsilon_m, min_samples, current_id):\n",
    "    eps = epsilon_m * 0.000009\n",
    "    data = current_id.copy()\n",
    "    \n",
    "    clustering = DBSCAN(\n",
    "        eps = eps, min_samples = min_samples).fit(data[['Long','Lat']])\n",
    "    \n",
    "    data.loc[:, 'clusterIndex'] = clustering.labels_\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_cluster(epsilon_m, rec_points):\n",
    "    eps = epsilon_m * 0.000009\n",
    "    data = rec_points.copy()\n",
    "    \n",
    "    add_clsut = AgglomerativeClustering(n_clusters=None, \n",
    "                                            linkage='single', \n",
    "                                                distance_threshold=eps).fit(data[['Long','Lat']])\n",
    "    \n",
    "    data.loc[:, 'rec_cluster_index'] = add_clsut.labels_\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan2_(epsilon_m2, min_samples2, rec_points):\n",
    "    eps = epsilon_m2 * 0.000009\n",
    "    data = rec_points.copy()\n",
    "    \n",
    "    clustering2 = DBSCAN(\n",
    "        eps = eps, min_samples = min_samples2).fit(data[['Long','Lat']])\n",
    "    \n",
    "    data.loc[:, 'clusterIndex2'] = clustering2.labels_\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if data frame became large u have to change block size\n",
    "raw_data = pd.read_csv(\n",
    "    r'C:\\Users\\Rahmani\\Desktop\\MobileData\\Data\\Data20000fix.csv',\n",
    "    parse_dates=[0])\n",
    "\n",
    "\n",
    "stay_points = pd.read_csv(r'C:\\Users\\Rahmani\\Desktop\\MobileData\\Data\\3000_staypoint.csv', \n",
    "                          parse_dates=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tehran_shp = gpd.read_file(r'C:\\Users\\Rahmani\\Desktop\\MobileData\\TehranGeorefed\\Tehran\\tehran.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading Tehran Shapefile\n",
    "tehran_manategh = gpd.read_file(r'C:\\Users\\Rahmani\\Desktop\\MobileData\\Tehran_SHP_New\\Manategh_22\\manategh.shp',crs={'init': 'epsg:4326'})\n",
    "tehran_manategh.to_crs(epsg=4326, inplace=True)\n",
    "tehran_manategh.drop(tehran_manategh.columns[[-2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13]], axis=1, inplace = True)\n",
    "tehran_manategh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Tehran Shapefile\n",
    "tehran_navahi = gpd.read_file(r'C:\\Users\\Rahmani\\Desktop\\MobileData\\Tehran_SHP_New\\Navahi_124\\navahi.shp')\n",
    "tehran_navahi.to_crs(epsg=4326, inplace=True)\n",
    "tehran_navahi.drop(tehran_navahi.columns[[-4, -5, -6, -7]], axis=1, inplace = True)\n",
    "tehran_navahi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding noise\n",
    "raw_data['noisy_lat'] = raw_data['Lat'].apply(add_noise)\n",
    "raw_data['noisy_long'] = raw_data['Long'].apply(add_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.loc[:, 'hour'] = raw_data.loc[:,'Date_Time'].dt.hour\n",
    "raw_data.loc[:, 'weekday'] = raw_data.loc[:,'Date_Time'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.sort_values(by=['ID', 'Date_Time'], inplace=True)\n",
    "raw_data.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Special IDs\n",
    "Karaj:\n",
    "ID: 19\n",
    "\n",
    "weird clusters:\n",
    "ID: 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stay_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data_groupby = raw_data.groupby(by=['ID'])\n",
    "counter = 0\n",
    "clustered_data_by_id = {}\n",
    "homeless_people = {}\n",
    "workless_people = {}\n",
    "recreation_points = pd.DataFrame()\n",
    "home_locations_by_id = {}\n",
    "work_locations_by_id = {}\n",
    "\n",
    "for ID in raw_data.ID.unique():\n",
    "    counter += 1\n",
    "    current_id = raw_data_groupby.get_group(ID)\n",
    "    current_id_clustered = dbscan_(1200, 7, current_id)\n",
    "    current_id_clustered.loc[:, 'clust_label'] = 'Not Known'\n",
    "    #home = current_id_clustered.loc[current_id_clustered['weekday'] != 'Friday'][['clusterIndex']].between_time('23:30', '06:00').mode()\n",
    "    #current_id_clusters_pivot = current_id_clustered.groupby(['clusterIndex'])[['ID']].count().sort_values('ID', ascending=False)\n",
    "    # ===============================================\n",
    "    # Finding home cluster\n",
    "    index = pd.DatetimeIndex(current_id_clustered['Date_Time'].loc[(current_id_clustered['weekday'] != 'Friday')])\n",
    "    home_clust = current_id_clustered.loc[(\n",
    "                    current_id_clustered['weekday'] != 'Friday')][['clusterIndex']].iloc[\n",
    "                        index.indexer_between_time('00:00','06:00')].mode()\n",
    "    # Labeling home cluster\n",
    "    if not home_clust.empty:\n",
    "        current_id_clustered.loc[current_id_clustered['clusterIndex'] == home_clust.iat[0,0], 'clust_label'] = 'Home'\n",
    "    else:\n",
    "        homeless_people['{}'.format(ID)] = current_id_clustered\n",
    "    \n",
    "    # ===============================================\n",
    "    # Finding work cluster\n",
    "    index = pd.DatetimeIndex(current_id_clustered['Date_Time'].loc[\n",
    "        (current_id_clustered['weekday'] != 'Friday') \n",
    "            & ((current_id_clustered['clust_label'] != 'Home'))])\n",
    "    \n",
    "    work_clust = current_id_clustered.loc[(\n",
    "                    current_id_clustered['weekday'] != 'Friday') \n",
    "                        & (current_id_clustered['clust_label'] != 'Home')][['clusterIndex']].iloc[\n",
    "                            index.indexer_between_time('09:00','17:00')].mode()\n",
    "    # Labeling work cluster\n",
    "    if not work_clust.empty:\n",
    "        if (current_id_clustered.clusterIndex.values == work_clust.iat[0,0]).sum() > 8:\n",
    "            current_id_clustered.loc[current_id_clustered['clusterIndex'] == work_clust.iat[0,0], 'clust_label'] = 'Work'\n",
    "    else:\n",
    "        workless_people['{}'.format(ID)] = current_id_clustered\n",
    "    # ===============================================\n",
    "    # Calculating average location for home and work\n",
    "    current_id_hw_loc = current_id_clustered.groupby('clust_label')[['Lat', 'Long']].mean()\n",
    "    current_id_hw_loc.reset_index(inplace = True)\n",
    "    current_id_home_loc = current_id_hw_loc.loc[current_id_hw_loc['clust_label'] == 'Home']\n",
    "    current_id_work_loc = current_id_hw_loc.loc[current_id_hw_loc['clust_label'] == 'Work']\n",
    "    home_locations_by_id['{}'.format(ID)] = current_id_home_loc\n",
    "    work_locations_by_id['{}'.format(ID)] = current_id_work_loc\n",
    "\n",
    "    # ===============================================\n",
    "    clustered_data_by_id['{}'.format(ID)] = current_id_clustered\n",
    "    \n",
    "    # Recreational Filter \n",
    "    index = pd.DatetimeIndex(current_id_clustered['Date_Time'].loc[\n",
    "        ((current_id_clustered['weekday'] == 'Friday') | (current_id_clustered['weekday'] == 'Thursday'))\n",
    "            & (current_id_clustered['clust_label'] != 'Home') \n",
    "                & (current_id_clustered['clust_label'] != 'Work')])\n",
    "    \n",
    "    current_id_recreation_points = current_id_clustered.loc[\n",
    "                    ((current_id_clustered['weekday'] == 'Friday') | (current_id_clustered['weekday'] == 'Thursday'))\n",
    "                        & (current_id_clustered['clust_label'] != 'Home') \n",
    "                            & (current_id_clustered['clust_label'] != 'Work')].iloc[\n",
    "                                index.indexer_between_time('19:00','23:59')]\n",
    "    if not current_id_recreation_points.empty:\n",
    "        if not home_clust.empty:\n",
    "            current_id_recreation_points = current_id_recreation_points.assign(home_lat= current_id_home_loc.at[0, 'Lat'], \n",
    "                                                                               home_long= current_id_home_loc.at[0, 'Long'])\n",
    "            recreation_points = recreation_points.append(current_id_recreation_points, ignore_index=True)\n",
    "\n",
    "#     if counter == 2:\n",
    "#         break;\n",
    "        \n",
    "\n",
    "# for key in clustered_data_by_Id:\n",
    "#     clusters_sorted = copy.copy(clustered_data_by_Id[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreation_points.to_csv('all_recreation_points.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Recreational Points to indentify hotspot areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreation_points_clustered = dbscan_(500, 100, recreation_points) # DBCAN does not work well here.\n",
    "recreation_points_clustered = agg_cluster(500, recreation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the GeoPandas dartaframe (shapefile)\n",
    "recreation_points_clustered_shp = gpd.GeoDataFrame(recreation_points_clustered, \n",
    "                                                   geometry = gpd.points_from_xy(recreation_points_clustered.Long, \n",
    "                                                                                 recreation_points_clustered.Lat), \n",
    "                                                                                   crs={'init': 'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort clusters based on their number of members\n",
    "recreation_points_clustered_pivot = recreation_points_clustered_shp.groupby(['rec_cluster_index'])[['ID']].count().sort_values('ID', ascending=False)\n",
    "\n",
    "# recreation_points_clustered.groupby(['clusterIndex2']).agg(['count']) # if you use DBSCAN here, you  should change to clusters index columns\n",
    "# in order to prevent overwriting new cluster indeces on previous indeces from home-work clustering session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering top_n clusters\n",
    "\n",
    "top_rec_points = recreation_points_clustered_pivot.head(10) # You can change 10 to X to find the top X areas\n",
    "top_rec_points.reset_index(inplace=True) # index to column \n",
    "top_10_areas = recreation_points_clustered_shp.loc[(recreation_points_clustered_shp.rec_cluster_index.isin(top_rec_points['rec_cluster_index']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ploting top_n areas on the map\n",
    "base = tehran_manategh.plot(color='white', edgecolor='black')\n",
    "top_10_areas.plot(ax=base, marker='o', column='rec_cluster_index', markersize=5, categorical=True, legend = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving top_n areas into shp and gpkg\n",
    "top_10_areas.to_file(\"recreation_points_10top_20000.gpkg\", layer='rec_points', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to CSV\n",
    "top_10_areas.to_csv(\"recreation_points_10top_20000.csv\", mode='w', columns=['Date_Time','ID','Lat','Long','hour','weekday','home_lat','home_long', 'rec_cluster_index'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreational Points Spatial Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_areas = pd.read_csv(\"recreation_points_10top_20000.csv\", parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_10_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the location of each recreation cluster members with the centroid of the cluster\n",
    "# By centroid we mean the average location of all cluter members\n",
    "\n",
    "top_10_areas['Long'] = top_10_areas.groupby(['rec_cluster_index'])['Long'].transform('mean') \n",
    "top_10_areas['Lat'] = top_10_areas.groupby(['rec_cluster_index'])['Lat'].transform('mean') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_areas_lat_longs = top_10_areas.groupby('rec_cluster_index')[['Lat', 'Long']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_10_areas_lat_longs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manategh 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_10_areas_shp = gpd.GeoDataFrame(top_10_areas, \n",
    "                                        geometry = gpd.points_from_xy(top_10_areas.Long, \n",
    "                                                                        top_10_areas.Lat), \n",
    "                                                                            crs={'init': 'epsg:4326'})\n",
    "top_10_areas_shp.set_crs(epsg=4326, inplace=True, allow_override=True)\n",
    "top_10_areas_in_manategh = gpd.sjoin(top_10_areas_shp, tehran_manategh, how=\"left\", op='within')\n",
    "top_10_areas_in_manategh.rename(columns={\"IDMAN\": \"recreation_mantaghe\"}, inplace = 'True')\n",
    "top_10_areas_in_manategh.drop(columns=['index_right'], inplace = True) \n",
    "\n",
    "top_10_areas_in_manategh = gpd.GeoDataFrame(top_10_areas_in_mahalat, \n",
    "                                        geometry = gpd.points_from_xy(top_10_areas_in_manategh.home_long, \n",
    "                                                                        top_10_areas_in_manategh.home_lat), \n",
    "                                                                            crs={'init': 'epsg:4326'})\n",
    "top_10_areas_in_manategh.set_crs(epsg=4326, inplace=True, allow_override=True)\n",
    "\n",
    "top_10_areas_in_manategh = gpd.sjoin(top_10_areas_in_mahalat, tehran_manategh, how=\"left\", op='within')\n",
    "top_10_areas_in_manategh.rename(columns={\"IDMAN\": \"home_mantaghe\"}, inplace = 'True')\n",
    "top_10_areas_in_manategh.drop(columns=['index_right'], inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navahi 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_areas_shp = gpd.GeoDataFrame(top_10_areas, \n",
    "                                        geometry = gpd.points_from_xy(top_10_areas.Long, \n",
    "                                                                        top_10_areas.Lat), \n",
    "                                                                            crs={'init': 'epsg:4326'})\n",
    "top_10_areas_shp.set_crs(epsg=4326, inplace=True, allow_override=True)\n",
    "top_10_areas_in_navahi = gpd.sjoin(top_10_areas_shp, tehran_navahi, how=\"left\", op='within')\n",
    "top_10_areas_in_navahi.rename(columns={\"NAVAHI\": \"recreation_mantaghe\"}, inplace = 'True')\n",
    "top_10_areas_in_navahi.drop(columns=['index_right'], inplace = True) \n",
    "\n",
    "top_10_areas_in_navahi = gpd.GeoDataFrame(top_10_areas_in_navahi, \n",
    "                                        geometry = gpd.points_from_xy(top_10_areas_in_navahi.home_long, \n",
    "                                                                        top_10_areas_in_navahi.home_lat), \n",
    "                                                                            crs={'init': 'epsg:4326'})\n",
    "top_10_areas_in_navahi.set_crs(epsg=4326, inplace=True, allow_override=True)\n",
    "\n",
    "top_10_areas_in_navahi = gpd.sjoin(top_10_areas_in_navahi, tehran_navahi, how=\"left\", op='within')\n",
    "top_10_areas_in_navahi.rename(columns={\"NAVAHI\": \"home_mantaghe\"}, inplace = 'True')\n",
    "top_10_areas_in_navahi.drop(columns=['index_right'], inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = pd.pivot_table(top_10_areas_in_navahi, values='NAVAHI_right', index=['NAVAHI_left'],\n",
    "#                     columns=['NAVAHI_right'], aggfunc=np.sum)\n",
    "recreation_OD = top_10_areas_in_navahi.pivot_table(index='home_mantaghe',\n",
    "                    columns='recreation_mantaghe', values = 'ID', aggfunc=len, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = pd.DataFrame(recreation_OD.stack())\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.merge(vector, tehran_navahi[['NAVAHI','Lat','Long']], left_on = ['home_mantaghe'],\n",
    "                   right_on = ['NAVAHI'], \n",
    "                   how = 'left')\n",
    "b = pd.merge(vector, tehran_navahi[['NAVAHI','Lat','Long']], left_on = ['recreation_mantaghe'],\n",
    "                   right_on = ['NAVAHI'], \n",
    "                   how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.concat([a, b], axis=1, sort=False)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.to_csv('vectorized.csv')\n",
    "recreation_OD.to_csv('od.csv')\n",
    "c.to_csv('keplger_input_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting Stay points for Current ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating Person shapefile\n",
    "current_id_points = gpd.GeoDataFrame(\n",
    "        current_id_clustered, geometry = gpd.points_from_xy(\n",
    "                                            current_id_clustered.Long, \n",
    "                                                current_id_clustered.Lat),  \n",
    "                                                    crs={'init': 'epsg:4326'})\n",
    "\n",
    "base = tehran_manategh.plot(color='white', edgecolor='black')\n",
    "current_id_points.plot(ax=base, marker='o', column='clusterIndex', markersize=5, categorical=True, legend = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating Noisy Points Shapefile\n",
    "# current_Id_clustered['noisy_lat'] = current_Id_clustered['Lat'].apply(add_noise)\n",
    "# current_Id_clustered['noisy_long'] = current_Id_clustered['Long'].apply(add_noise)\n",
    "\n",
    "current_id_clustered_points = gpd.GeoDataFrame(\n",
    "    current_id_clustered, geometry = gpd.points_from_xy(\n",
    "        current_id_clustered.noisy_long, current_id_clustered.noisy_lat),  crs={'init': 'epsg:4326'})\n",
    "\n",
    "base = tehran_manategh.plot(color='white', edgecolor='black')\n",
    "current_id_clustered_points.plot(ax=base, marker='o', column='clusterIndex', markersize=5, categorical=True, legend = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting the cluster time histogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the weekdays in order to seperate workdays and weekdays\n",
    "current_id_clustered['weekday'] = current_id_clustered[['Date_Time']].apply(lambda x: dt.datetime.strftime(x['Date_Time'], '%A'), axis=1)\n",
    "\n",
    "# Defining the cluster number for which we want to plot the histogram\n",
    "desired_cluster = copy.copy(current_id_clustered[current_id_clustered['clusterIndex'] == 2])\n",
    "\n",
    "# Filtering Workdays\n",
    "desired_cluster_workdays = copy.copy(desired_cluster[(desired_cluster['weekday'] != 'Friday') & (desired_cluster['weekday'] != 'Thursday')])\n",
    "# Filtering Weekends\n",
    "desired_cluster_weekends = copy.copy(desired_cluster[(desired_cluster['weekday'] == 'Friday')])\n",
    "# all days\n",
    "desired_cluster_all_days = copy.copy(desired_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hour of connection\n",
    "desired_cluster_all_days.loc[:, 'hour'] = desired_cluster_all_days.loc[:,'Date_Time'].dt.hour\n",
    "\n",
    "# Defining the hour of connection\n",
    "desired_cluster_workdays.loc[:, 'hour'] = desired_cluster_workdays.loc[:,'Date_Time'].dt.hour\n",
    "\n",
    "# Defining the hour of connection\n",
    "desired_cluster_weekends.loc[:, 'hour'] = desired_cluster_weekends.loc[:,'Date_Time'].dt.hour\n",
    "\n",
    "# Ploting the histogram\n",
    "desired_cluster_all_days.hist(column='hour', bins=23, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)\n",
    "plt.title('All Days Histogram')\n",
    "\n",
    "# Ploting the histogram\n",
    "desired_cluster_workdays.hist(column='hour', bins=23, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)\n",
    "plt.title('Work Days Histogram')\n",
    "\n",
    "desired_cluster_weekends.hist(column='hour', bins=23, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)\n",
    "plt.title('Weekends Histogram')\n",
    "# Method 2: plt.hist(desired_cluster['hour'], bins = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cluster_workdays.groupby(['hour']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Person shapefile\n",
    "current_ID_Points = gpd.GeoDataFrame(\n",
    "        current_Id_clustered, geometry = gpd.points_from_xy(\n",
    "                                            current_Id_clustered.Long, \n",
    "                                                current_Id_clustered.Lat),  \n",
    "                                                    crs={'init': 'epsg:4326'})\n",
    "\n",
    "base = tehran_manategh.plot(color='white', edgecolor='black')\n",
    "current_ID_Points.plot(ax=base, marker='o', column='clusterIndex', markersize=5, categorical=True, legend = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating Noisy Points Shapefile\n",
    "current_Id_clustered['noisy_lat'] = current_Id_clustered['Lat'].apply(add_noise)\n",
    "current_Id_clustered['noisy_long'] = current_Id_clustered['Long'].apply(add_noise)\n",
    "\n",
    "current_Id_clustered_Points = gpd.GeoDataFrame(\n",
    "    current_Id_clustered, geometry = gpd.points_from_xy(\n",
    "        current_Id_clustered.noisy_long, current_Id_clustered.noisy_lat),  crs={'init': 'epsg:4326'})\n",
    "\n",
    "base = tehran_manategh.plot(color='white', edgecolor='black')\n",
    "current_Id_clustered_Points.plot(ax=base, marker='o', column='clusterIndex', markersize=5, categorical=True, legend = True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
